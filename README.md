# german-moe-gpt-v8
Dieses Repository enth√§lt die Skripte und Konfigurationen zum Trainieren des deutschen 149.6M Parameter Mixture-of-Experts (MoE)  Sprachmodells.
